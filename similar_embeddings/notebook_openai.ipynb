{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "from scipy.special import softmax\n",
    "\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"openai version: {openai.__version__}\")\n",
    "print(f\"scipy version: {scipy.__version__}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Set your OpenAI API key before running the below cell",
   "id": "9d7ac2015777f759"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "api_key = ''\n",
    "client = OpenAI(api_key=api_key)"
   ],
   "id": "79ff701249617024",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_embeddings(texts, model=\"text-embedding-ada-002\"):\n",
    "    \"\"\"\n",
    "    Fetches the embedding for a given text using OpenAI's embedding model.\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=texts,\n",
    "    )\n",
    "    return [np.array(data.embedding) for data in response.data]\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n"
   ],
   "id": "1b29df629d76032d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# List of words to get embeddings for\n",
    "other_words = [\"queen\", \"princess\", \"monarch\", \"empress\", \"duchess\", \"lady\", \"baseball\", \"floor\", \"monitor\"]\n",
    "words = [\"king\", \"man\", \"woman\"] + other_words\n",
    "\n",
    "# Get all embeddings in one API call\n",
    "embeddings = get_embeddings(words)\n",
    "\n",
    "# Create a mapping from word to embedding\n",
    "word_to_embedding = dict(zip(words, embeddings))\n",
    "\n",
    "# Extract embeddings\n",
    "embedding_king = word_to_embedding[\"king\"]\n",
    "embedding_man = word_to_embedding[\"man\"]\n",
    "embedding_woman = word_to_embedding[\"woman\"]\n",
    "\n",
    "# Perform the vector arithmetic\n",
    "result_vector = embedding_king - embedding_man + embedding_woman\n",
    "\n",
    "# Calculate cosine similarities with other words\n",
    "similarities = []\n",
    "print(\"Cosine Similarities:\")\n",
    "for word in other_words:\n",
    "    embedding_word = word_to_embedding[word]\n",
    "    similarity = cosine_similarity(result_vector, embedding_word)\n",
    "    similarities.append(similarity)\n",
    "    print(f\"Similarity with '{word}': {similarity:.4f}\")\n",
    "\n",
    "min_similarity = min(similarities)\n",
    "shifted_similarities = [sim - min_similarity for sim in similarities]\n",
    "\n",
    "# Apply softmax to the scaled similarities\n",
    "temperature = 0.01\n",
    "scaled_similarities = [sim / temperature for sim in shifted_similarities]\n",
    "probabilities = softmax(scaled_similarities)\n",
    "\n",
    "# Print the softmax probabilities\n",
    "print(\"\\nSoftmax Probabilities:\")\n",
    "for word, probability in zip(other_words, probabilities):\n",
    "    print(f\"Probability for '{word}': {probability:.4f}\")\n"
   ],
   "id": "34f2146c6870b3b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# On Your Own\n",
    "\n",
    "1. Think of another embedding arithmetic problem that follows the pattern: Word A - Word B + Word C = ?\n",
    "2. Calculate the embeddings for the words in the problem above\n",
    "3. Create a list of words that you think are similar to the words in the problem above\n",
    "4. Calculate the cosine similarity between the embedding of the words in the problem above and the embeddings of the similar words\n",
    "5. Use the cosine similarity to calculate the probabilities of the similar words\n",
    "6. Print the probabilities"
   ],
   "id": "1924f3ab69e6c62c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "453431204a90aac7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
